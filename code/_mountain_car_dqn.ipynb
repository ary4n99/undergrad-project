{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "-qMlcpmuSmwK"
            },
            "outputs": [],
            "source": [
                "import collections\n",
                "import random\n",
                "import warnings\n",
                "\n",
                "import gym\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "\n",
                "import utils\n",
                "\n",
                "keras = tf.keras\n",
                "\n",
                "from keras.layers import Dense, Input\n",
                "from keras.optimizers import Adam\n",
                "\n",
                "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GAME = \"mountain_car_dqn\"\n",
                "VERBOSITY = \"0\"\n",
                "SAVE_FREQUENCY = 25"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "tUkQyIFaSq8M"
            },
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "EPISODES = 500\n",
                "LEARNING_RATE = 0.001\n",
                "GAMMA = 0.99\n",
                "\n",
                "EPSILON = 1\n",
                "EPSILON_DECAY = 0.99\n",
                "EPSILON_MINIMUM = 0.01\n",
                "ACTION_PROBABILITIES = [0.4, 0.2, 0.4]  # left, nothing, right\n",
                "\n",
                "BATCH_SIZE = 32\n",
                "REPLAY_BUFFER_SIZE = 100000"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "CBwZ8nGiSq15"
            },
            "outputs": [],
            "source": [
                "def instantiate_model(env):\n",
                "    input = Input(shape=(env.observation_space.shape))\n",
                "    dense1 = Dense(32, activation=\"relu\")(input)\n",
                "    dense2 = Dense(64, activation=\"relu\")(dense1)\n",
                "    output = Dense(env.action_space.n, activation=\"linear\")(dense2)\n",
                "    model = keras.Model(inputs=input, outputs=output)\n",
                "\n",
                "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
                "\n",
                "    return model\n",
                "\n",
                "\n",
                "def take_action(env, action):\n",
                "    next_state, reward, done, _ = env.step(action)\n",
                "    return next_state, reward, done\n",
                "\n",
                "\n",
                "def shape_reward(state, next_state, reward):\n",
                "    return reward + 300 * (abs(next_state[1]) - abs(state[1]))\n",
                "\n",
                "\n",
                "def train_on_batch(model, replay_buffer):\n",
                "    batch = random.sample(replay_buffer, BATCH_SIZE)\n",
                "\n",
                "    states = np.array([x[0] for x in batch])\n",
                "    actions = np.array([x[1] for x in batch])\n",
                "    rewards = np.array([x[2] for x in batch])\n",
                "    next_states = np.array([x[3] for x in batch])\n",
                "    dones = np.array([x[4] for x in batch])\n",
                "\n",
                "    targets = rewards + GAMMA * np.amax(\n",
                "        np.squeeze(model.predict_on_batch(next_states)), axis=1) * (1 - dones)\n",
                "    targets_full = np.squeeze(model.predict_on_batch(states))\n",
                "    targets_full[np.arange(BATCH_SIZE), actions] = targets\n",
                "\n",
                "    model.fit(states, targets_full, verbose=VERBOSITY)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "EH9723DLQXEd"
            },
            "outputs": [],
            "source": [
                "env = gym.make(\"MountainCar-v0\", new_step_api=False)\n",
                "\n",
                "model = instantiate_model(env)\n",
                "\n",
                "replay_buffer = collections.deque(maxlen=REPLAY_BUFFER_SIZE)\n",
                "\n",
                "reward_history = []\n",
                "\n",
                "# Training\n",
                "for episode in range(EPISODES + 1):\n",
                "    state = env.reset()\n",
                "    episode_reward = 0\n",
                "    done = False\n",
                "\n",
                "    # Episode loop\n",
                "    while not done:\n",
                "        if np.random.uniform(0, 1) < EPSILON:\n",
                "            action = np.random.choice(3, p=ACTION_PROBABILITIES)\n",
                "        else:\n",
                "            action = np.argmax(model(np.expand_dims(state, axis=0)))\n",
                "\n",
                "        next_state, reward, done = take_action(env, action)\n",
                "        reward = shape_reward(state, next_state, reward)\n",
                "        episode_reward += reward\n",
                "\n",
                "        # Store transition in replay buffer\n",
                "        replay_buffer.append((state, action, reward, next_state, done))\n",
                "\n",
                "        state = next_state\n",
                "\n",
                "        # Sample batch and update model\n",
                "        if len(replay_buffer) >= BATCH_SIZE:\n",
                "            train_on_batch(model, replay_buffer)\n",
                "\n",
                "    EPSILON *= EPSILON_DECAY\n",
                "    EPSILON = max(EPSILON_MINIMUM, EPSILON)\n",
                "\n",
                "    reward_history.append(episode_reward)\n",
                "\n",
                "    utils.save_progress(model, reward_history, episode + 1, SAVE_FREQUENCY,\n",
                "                        GAME)\n",
                "\n",
                "    utils.log(episode, episode_reward, EPSILON)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "private_outputs": true,
            "provenance": []
        },
        "gpuClass": "standard",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.16"
        },
        "vscode": {
            "interpreter": {
                "hash": "b946e2faa49f4674d4dbe235d6e8a6770d62cc3857e24103415cecb3f0034c27"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
