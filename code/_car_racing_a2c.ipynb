{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "kcqqWlnsOLpx"
            },
            "outputs": [],
            "source": [
                "import collections\n",
                "import warnings\n",
                "\n",
                "import gym\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "\n",
                "import utils\n",
                "\n",
                "keras = tf.keras\n",
                "\n",
                "from keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D\n",
                "from keras.optimizers import Adam\n",
                "\n",
                "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GAME = \"car_racing_a2c\"\n",
                "VERBOSITY = \"0\"\n",
                "SAVE_FREQUENCY = 25"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "0ajucomKOMaS"
            },
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "EPISODES = 500\n",
                "LEARNING_RATE = 0.00001\n",
                "GAMMA = 0.95\n",
                "\n",
                "FRAME_SKIP = 2\n",
                "FRAME_STACK_SIZE = 3\n",
                "NEGATIVE_REWARD_BREAK = 100\n",
                "\n",
                "ENTROPY_COEFFICIENT = 0.1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "FOo9U3fBP0pw"
            },
            "outputs": [],
            "source": [
                "def instantiate_model(env):\n",
                "    input = Input(shape=(84, 84, FRAME_STACK_SIZE))\n",
                "\n",
                "    conv1 = Conv2D(filters=8,\n",
                "                   kernel_size=(7, 7),\n",
                "                   strides=4,\n",
                "                   activation=\"relu\",\n",
                "                   kernel_regularizer='l2')(input)\n",
                "    maxp1 = MaxPooling2D(pool_size=(2, 2), strides=2)(conv1)\n",
                "    conv2 = Conv2D(filters=16,\n",
                "                   kernel_size=(3, 3),\n",
                "                   activation=\"relu\",\n",
                "                   kernel_regularizer='l2')(maxp1)\n",
                "    maxp2 = MaxPooling2D(pool_size=(2, 2), strides=2)(conv2)\n",
                "    flatten = Flatten()(maxp2)\n",
                "\n",
                "    dense = Dense(512, activation=\"relu\", kernel_regularizer='l2')(flatten)\n",
                "    output = Dense(env.action_space.n,\n",
                "                   activation=\"linear\",\n",
                "                   kernel_regularizer='l2')(dense)\n",
                "    actor = keras.Model(inputs=input, outputs=output)\n",
                "\n",
                "    dense = Dense(512, activation=\"relu\", kernel_regularizer='l2')(flatten)\n",
                "    output = Dense(1, activation=\"linear\", kernel_regularizer='l2')(dense)\n",
                "    critic = keras.Model(inputs=input, outputs=output)\n",
                "\n",
                "    return actor, critic\n",
                "\n",
                "\n",
                "def take_action(env, action):\n",
                "    reward = 0\n",
                "    for _ in range(FRAME_SKIP + 1):\n",
                "        next_state, frame_reward, done, _ = env.step(action)\n",
                "        reward += frame_reward\n",
                "        if done:\n",
                "            break\n",
                "    return next_state, reward, done\n",
                "\n",
                "\n",
                "def update_weights(actor, critic, optimizer, tape, state, next_state, reward,\n",
                "                   done, action_prob, action_log_prob):\n",
                "    advantage = reward + (1 - done) * GAMMA * critic(\n",
                "        np.expand_dims(next_state, axis=0)) - critic(\n",
                "            np.expand_dims(state, axis=0))\n",
                "\n",
                "    critic_loss = tf.math.pow(advantage, 2)\n",
                "    grads = tape.gradient(critic_loss, critic.trainable_variables)\n",
                "    optimizer.apply_gradients(zip(grads, critic.trainable_variables))\n",
                "\n",
                "    actor_loss = -action_log_prob * advantage - ENTROPY_COEFFICIENT * -(\n",
                "        action_log_prob * action_prob)\n",
                "    grads = tape.gradient(actor_loss, actor.trainable_variables)\n",
                "    optimizer.apply_gradients(zip(grads, actor.trainable_variables))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "2s9XGpXmkXAW"
            },
            "outputs": [],
            "source": [
                "env = gym.make(\"CarRacing-v2\", new_step_api=False, continuous=False)\n",
                "\n",
                "actor, critic = instantiate_model(env)\n",
                "\n",
                "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
                "\n",
                "reward_history = []\n",
                "\n",
                "# Training\n",
                "for episode in range(EPISODES + 1):\n",
                "    state = utils.CarRacing.process_state(env.reset())\n",
                "    episode_reward = 0\n",
                "    done = False\n",
                "\n",
                "    frame_stack = collections.deque([state] * FRAME_STACK_SIZE,\n",
                "                                    maxlen=FRAME_STACK_SIZE)\n",
                "    negative_reward_count = 0\n",
                "\n",
                "    # Episode loop\n",
                "    with tf.GradientTape(persistent=True) as tape:\n",
                "        while not done:\n",
                "            curr_frame_stack = utils.CarRacing.transpose_frame_stack(\n",
                "                frame_stack)\n",
                "\n",
                "            action_logits = actor(np.expand_dims(curr_frame_stack, axis=0))\n",
                "            action = tf.random.categorical(action_logits, 1)[0, 0]\n",
                "            action_probs = tf.nn.softmax(action_logits)\n",
                "\n",
                "            next_state, reward, done = take_action(env, int(action))\n",
                "            episode_reward += reward\n",
                "            negative_reward_count = negative_reward_count + 1 if reward < 0 else 0\n",
                "\n",
                "            frame_stack.append(utils.CarRacing.process_state(next_state))\n",
                "            next_frame_stack = utils.CarRacing.transpose_frame_stack(\n",
                "                frame_stack)\n",
                "\n",
                "            action_prob = action_probs[0, action]\n",
                "            action_log_prob = tf.math.log(action_probs[0, action])\n",
                "\n",
                "            if negative_reward_count > NEGATIVE_REWARD_BREAK:\n",
                "                break\n",
                "\n",
                "            update_weights(actor, critic, optimizer, tape, curr_frame_stack,\n",
                "                           next_frame_stack, reward, done, action_prob,\n",
                "                           action_log_prob)\n",
                "\n",
                "    reward_history.append(episode_reward)\n",
                "\n",
                "    utils.save_progress(actor, reward_history, episode + 1, SAVE_FREQUENCY,\n",
                "                        GAME)\n",
                "\n",
                "    utils.log(episode, episode_reward)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "private_outputs": true,
            "provenance": []
        },
        "gpuClass": "standard",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.16"
        },
        "vscode": {
            "interpreter": {
                "hash": "b946e2faa49f4674d4dbe235d6e8a6770d62cc3857e24103415cecb3f0034c27"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
