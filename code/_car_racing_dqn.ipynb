{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "kcqqWlnsOLpx"
            },
            "outputs": [],
            "source": [
                "import collections\n",
                "import random\n",
                "import warnings\n",
                "\n",
                "import gym\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "\n",
                "import utils\n",
                "\n",
                "keras = tf.keras\n",
                "\n",
                "from keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D\n",
                "from keras.optimizers import Adam\n",
                "\n",
                "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GAME = \"car_racing_dqn\"\n",
                "VERBOSITY = \"0\"\n",
                "SAVE_FREQUENCY = 25"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "0ajucomKOMaS"
            },
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "EPISODES = 500\n",
                "LEARNING_RATE = 0.001\n",
                "GAMMA = 0.95\n",
                "\n",
                "FRAME_SKIP = 2\n",
                "FRAME_STACK_SIZE = 3\n",
                "NEGATIVE_REWARD_BREAK = 100\n",
                "\n",
                "EPSILON = 1\n",
                "EPSILON_DECAY = 0.99\n",
                "EPSILON_MINIMUM = 0.01\n",
                "ACTION_PROBABILITIES = [0.0, 0.2, 0.2, 0.5,\n",
                "                        0.1]  # nothing, left, right, gas, brake\n",
                "\n",
                "BATCH_SIZE = 32\n",
                "REPLAY_BUFFER_SIZE = 5000"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "FOo9U3fBP0pw"
            },
            "outputs": [],
            "source": [
                "def instantiate_model(env):\n",
                "    input = Input(shape=(84, 84, FRAME_STACK_SIZE))\n",
                "    conv1 = Conv2D(filters=8,\n",
                "                   kernel_size=(7, 7),\n",
                "                   strides=4,\n",
                "                   activation=\"relu\",\n",
                "                   kernel_regularizer='l2')(input)\n",
                "    maxp1 = MaxPooling2D(pool_size=(2, 2), strides=2)(conv1)\n",
                "    conv2 = Conv2D(filters=16,\n",
                "                   kernel_size=(3, 3),\n",
                "                   activation=\"relu\",\n",
                "                   kernel_regularizer='l2')(maxp1)\n",
                "    maxp2 = MaxPooling2D(pool_size=(2, 2), strides=2)(conv2)\n",
                "    flatten = Flatten()(maxp2)\n",
                "    dense = Dense(512, activation=\"relu\", kernel_regularizer='l2')(flatten)\n",
                "    output = Dense(env.action_space.n,\n",
                "                   activation=\"linear\",\n",
                "                   kernel_regularizer='l2')(dense)\n",
                "    model = keras.Model(inputs=input, outputs=output)\n",
                "\n",
                "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
                "\n",
                "    return model\n",
                "\n",
                "\n",
                "def take_action(env, action):\n",
                "    reward = 0\n",
                "    for _ in range(FRAME_SKIP + 1):\n",
                "        next_state, frame_reward, done, _ = env.step(action)\n",
                "        reward += frame_reward\n",
                "        if done:\n",
                "            break\n",
                "    return next_state, reward, done\n",
                "\n",
                "\n",
                "def train_on_batch(model, replay_buffer):\n",
                "    batch = random.sample(replay_buffer, BATCH_SIZE)\n",
                "\n",
                "    states = np.array([x[0] for x in batch])\n",
                "    actions = np.array([x[1] for x in batch])\n",
                "    rewards = np.array([x[2] for x in batch])\n",
                "    next_states = np.array([x[3] for x in batch])\n",
                "    dones = np.array([x[4] for x in batch])\n",
                "\n",
                "    targets = rewards + GAMMA * np.amax(\n",
                "        np.squeeze(model.predict_on_batch(next_states)), axis=1) * (1 - dones)\n",
                "    targets_full = np.squeeze(model.predict_on_batch(states))\n",
                "    targets_full[np.arange(BATCH_SIZE), actions] = targets\n",
                "\n",
                "    model.fit(states, targets_full, verbose=VERBOSITY)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "2s9XGpXmkXAW"
            },
            "outputs": [],
            "source": [
                "env = gym.make(\"CarRacing-v2\", new_step_api=False, continuous=False)\n",
                "\n",
                "model = instantiate_model(env)\n",
                "\n",
                "replay_buffer = collections.deque(maxlen=REPLAY_BUFFER_SIZE)\n",
                "\n",
                "reward_history = []\n",
                "\n",
                "# Training\n",
                "for episode in range(EPISODES + 1):\n",
                "    state = utils.CarRacing.process_state(env.reset())\n",
                "    episode_reward = 0\n",
                "    done = False\n",
                "\n",
                "    frame_stack = collections.deque([state] * FRAME_STACK_SIZE,\n",
                "                                    maxlen=FRAME_STACK_SIZE)\n",
                "    negative_reward_count = 0\n",
                "\n",
                "    # Episode loop\n",
                "    while not done:\n",
                "        curr_frame_stack = utils.CarRacing.transpose_frame_stack(frame_stack)\n",
                "\n",
                "        if np.random.uniform(0, 1) < EPSILON:\n",
                "            action = np.random.choice(5, p=ACTION_PROBABILITIES)\n",
                "        else:\n",
                "            action = np.argmax(model(np.expand_dims(curr_frame_stack, axis=0)))\n",
                "\n",
                "        next_state, reward, done = take_action(env, action)\n",
                "        episode_reward += reward\n",
                "        negative_reward_count = negative_reward_count + 1 if reward < 0 else 0\n",
                "\n",
                "        frame_stack.append(utils.CarRacing.process_state(next_state))\n",
                "        next_frame_stack = utils.CarRacing.transpose_frame_stack(frame_stack)\n",
                "\n",
                "        # Store transition in replay buffer\n",
                "        replay_buffer.append(\n",
                "            (curr_frame_stack, action, reward, next_frame_stack, done))\n",
                "\n",
                "        if negative_reward_count > NEGATIVE_REWARD_BREAK:\n",
                "            break\n",
                "\n",
                "        # Sample batch and update model\n",
                "        if len(replay_buffer) >= BATCH_SIZE:\n",
                "            train_on_batch(model, replay_buffer)\n",
                "\n",
                "    EPSILON *= EPSILON_DECAY\n",
                "    EPSILON = max(EPSILON_MINIMUM, EPSILON)\n",
                "\n",
                "    reward_history.append(episode_reward)\n",
                "\n",
                "    utils.save_progress(model, reward_history, episode + 1, SAVE_FREQUENCY,\n",
                "                        GAME)\n",
                "\n",
                "    utils.log(episode, episode_reward, EPSILON)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "private_outputs": true,
            "provenance": []
        },
        "gpuClass": "standard",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.16"
        },
        "vscode": {
            "interpreter": {
                "hash": "b946e2faa49f4674d4dbe235d6e8a6770d62cc3857e24103415cecb3f0034c27"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
