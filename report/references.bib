@article{watkins1989learning,
  title     = {Learning from delayed rewards},
  author    = {Watkins, Christopher John Cornish Hellaby},
  year      = {1989},
  publisher = {King's College, Cambridge United Kingdom}
}
@article{skinner1971operant,
  title     = {Operant conditioning},
  author    = {Skinner, Burrhus F},
  year      = {1971},
  journal   = {The encyclopedia of education},
  publisher = {Macmillan and Free Press New York},
  volume    = {7},
  pages     = {29--33}
}
@article{sutton1988learning,
  title     = {Learning to predict by the methods of temporal differences},
  author    = {Sutton, Richard S},
  year      = {1988},
  journal   = {Machine learning},
  publisher = {Springer},
  volume    = {3},
  pages     = {9--44}
}
@article{tesauro1995temporal,
  title   = {Temporal difference learning and TD-Gammon},
  author  = {Tesauro, Gerald and others},
  year    = {1995},
  journal = {Communications of the ACM},
  volume  = {38},
  number  = {3},
  pages   = {58--68}
}
@book{sutton2018reinforcement,
  title     = {Reinforcement learning: An introduction},
  author    = {Sutton, Richard S and Barto, Andrew G},
  year      = {2018},
  publisher = {MIT press}
}
@article{silver2017mastering,
  title   = {Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author  = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  year    = {2017},
  journal = {arXiv preprint arXiv:1712.01815}
}
@article{schrittwieser2020mastering,
  title     = {Mastering atari, go, chess and shogi by planning with a learned model},
  author    = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  year      = {2020},
  journal   = {Nature},
  publisher = {Nature Publishing Group UK London},
  volume    = {588},
  number    = {7839},
  pages     = {604--609}
}
@article{silver2016mastering,
  title     = {Mastering the game of Go with deep neural networks and tree search},
  author    = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  year      = {2016},
  journal   = {nature},
  publisher = {Nature Publishing Group},
  volume    = {529},
  number    = {7587},
  pages     = {484--489}
}
@inproceedings{mnih2016asynchronous,
  title        = {Asynchronous methods for deep reinforcement learning},
  author       = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year         = {2016},
  booktitle    = {International conference on machine learning},
  pages        = {1928--1937},
  organization = {PMLR}
}
@article{mnih2013playing,
  title   = {Playing atari with deep reinforcement learning},
  author  = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year    = {2013},
  journal = {arXiv preprint arXiv:1312.5602}
}
@misc{silver2015lecture,
  title  = {Lectures on reinforcement learning},
  author = {David Silver},
  year   = {2015},
  url    = {https://www.davidsilver.uk/teaching/}
}
@article{schaul2015prioritized,
  title   = {Prioritized experience replay},
  author  = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  year    = {2015},
  journal = {arXiv preprint arXiv:1511.05952}
}
@article{rosenblatt1958perceptron,
  title     = {The perceptron: a probabilistic model for information storage and organization in the brain.},
  author    = {Rosenblatt, Frank},
  year      = {1958},
  journal   = {Psychological review},
  publisher = {American Psychological Association},
  volume    = {65},
  number    = {6},
  pages     = {386}
}
@article{mcfarlane2018survey,
  title   = {A survey of exploration strategies in reinforcement learning},
  author  = {McFarlane, Roger},
  year    = {2018},
  journal = {McGill University}
}
@article{markov1954theory,
  title     = {The theory of algorithms},
  author    = {Markov, Andrei Andreevich},
  year      = {1954},
  journal   = {Trudy Matematicheskogo Instituta Imeni VA Steklova},
  publisher = {Russian Academy of Sciences, Steklov Mathematical Institute of Russian~â€¦},
  volume    = {42},
  pages     = {3--375}
}
@inproceedings{ioffe2015batch,
  title        = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author       = {Ioffe, Sergey and Szegedy, Christian},
  year         = {2015},
  booktitle    = {International conference on machine learning},
  pages        = {448--456},
  organization = {pmlr}
}
@article{o2015introduction,
  title   = {An introduction to convolutional neural networks},
  author  = {O'Shea, Keiron and Nash, Ryan},
  year    = {2015},
  journal = {arXiv preprint arXiv:1511.08458}
}
@article{lillicrap2015continuous,
  title   = {Continuous control with deep reinforcement learning},
  author  = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year    = {2015},
  journal = {arXiv preprint arXiv:1509.02971}
}
@article{watkins1992q,
  title     = {Q-learning},
  author    = {Watkins, Christopher JCH and Dayan, Peter},
  year      = {1992},
  journal   = {Machine learning},
  publisher = {Springer},
  volume    = {8},
  pages     = {279--292}
}
@article{barto1983neuronlike,
  title     = {Neuronlike adaptive elements that can solve difficult learning control problems},
  author    = {Barto, Andrew G and Sutton, Richard S and Anderson, Charles W},
  year      = {1983},
  journal   = {IEEE transactions on systems, man, and cybernetics},
  publisher = {IEEE},
  number    = {5},
  pages     = {834--846}
}
@article{fuji2018deep,
  title    = {Deep multi-agent reinforcement learning using dnn-weight evolution to optimize supply chain performance},
  author   = {Fuji, Taiki and Ito, Kiyoto and Matsumoto, Kohsei and Yano, Kazuo},
  year     = {2018},
  keywords = {fig}
}
@misc{dertat2017applied,
  title     = {Applied Deep Learning - Part 4: Convolutional Neural Networks},
  author    = {Dertat, Arden},
  year      = {2017},
  journal   = {Towards Data Science},
  publisher = {Towards Data Science},
  url       = {https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2},
  keywords  = {fig}
}
@misc{freecodecamp2018an,
  title    = {An intro to Advantage Actor Critic methods},
  year     = {2018},
  author   = {Writer, freeCodeCamp},
  journal  = {freeCodeCamp.org},
  url      = {https://freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/},
  keywords = {fig}
}
@article{cardenas2022complex,
  title     = {Complex Color Space Segmentation to Classify Objects in Urban Environments},
  author    = {Cardenas-Cornejo, Juan-Jose and Ibarra-Manzano, Mario-Alberto and Razo-Medina, Daniel-Alberto and Almanza-Ojeda, Dora-Luz},
  journal   = {Mathematics},
  volume    = {10},
  number    = {20},
  pages     = {3752},
  year      = {2022},
  publisher = {MDPI},
  keywords = {fig}
}
@misc{apple0000blurring,
  title    = {Blurring an Image},
  author   = {Developer, Apple},
  journal  = {developer.apple.com},
  url      = {https://developer.apple.com/documentation/accelerate/blurring_an_image},
  keywords = {fig}
}
@misc{jakhar2020reinforcement,
  title    = {Reinforcement Learning: Cart-pole, Deep Q-learning},
  author   = {Jakhar, Karan},
  year     = {2020},
  journal  = {Medium},
  url      = {https://karan-jakhar.medium.com/100-days-of-code-day-4-6fbc672171e4},
  keywords = {fig}
}
@inproceedings{montoya2021decoupling,
  title     = {Decoupling State Representation Methods from Reinforcement Learning in Car Racing.},
  author    = {Montoya, Juan M and Daunhawer, Imant and Vogt, Julia E and Wiering, Marco A},
  year      = {2021},
  booktitle = {ICAART (2)},
  pages     = {752--759},
  keywords  = {fig}
}
@misc{ledell2021statistical,
  title    = {Statistical Learning \& Data Mining IV},
  author   = {LeDell, Erin},
  year     = {2021},
  journal  = {GitHub},
  url      = {https://github.com/ledell/sldm4-h2o},
  keywords = {fig}
}
@article{csaji2001approximation,
  title    = {Approximation with artificial neural networks},
  author   = {Cs{\'a}ji, Bal{\'a}zs Csan{\'a}d and others},
  year     = {2001},
  journal  = {Faculty of Sciences, Etvs Lornd University, Hungary},
  volume   = {24},
  number   = {48},
  pages    = {7},
  keywords = {fig}
}
@misc{vidhya2019introduction,
  title    = {Introduction to Deep Q-Learning for Reinforcement Learning (in Python)},
  author   = {Analytics Vidhya},
  year     = {2019},
  journal  = {Analytics Vidhya},
  url      = {https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/},
  keywords = {fig}
}
@misc{erainnovator2021reinforcement,
  title    = {Reinforcement Learning (RL) | Markov Decision Process | Q-Learning},
  author   = {EraInnovator},
  year     = {2021},
  journal  = {EraInnovator},
  url      = {https://erainnovator.com/reinforcement-learning/},
  keywords = {fig}
}
@misc{mcleod2023b,
  title    = {B.F. Skinner | Operant Conditioning | Simply Psychology},
  author   = {Mcleod, Saul},
  year     = {2023},
  journal  = {www.simplypsychology.org},
  url      = {https://simplypsychology.org/operant-conditioning.html},
  keywords = {fig}
}
@article{kingma2014adam,
  title   = {Adam: A method for stochastic optimization},
  author  = {Kingma, Diederik P and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  year    = {2014}
}
@article{andrychowicz2017hindsight,
  title   = {Hindsight experience replay},
  author  = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}
@article{wu2017scalable,
  title   = {Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation},
  author  = {Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}
@article{sutton1999policy,
  title   = {Policy gradient methods for reinforcement learning with function approximation},
  author  = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal = {Advances in neural information processing systems},
  volume  = {12},
  year    = {1999}
}
@article{simonyan2014very,
  title   = {Very deep convolutional networks for large-scale image recognition},
  author  = {Simonyan, Karen and Zisserman, Andrew},
  journal = {arXiv preprint arXiv:1409.1556},
  year    = {2014}
}
@article{bellemare2013arcade,
  title   = {The arcade learning environment: An evaluation platform for general agents},
  author  = {Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {47},
  pages   = {253--279},
  year    = {2013}
}
@inproceedings{fujimoto2018addressing,
  title        = {Addressing function approximation error in actor-critic methods},
  author       = {Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle    = {International conference on machine learning},
  pages        = {1587--1596},
  year         = {2018},
  organization = {PMLR}
}
@inproceedings{haarnoja2018soft,
  title        = {Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author       = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle    = {International conference on machine learning},
  pages        = {1861--1870},
  year         = {2018},
  organization = {PMLR}
}
@misc{brockman2016gym,
  author  = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  title   = {OpenAI Gym},
  year    = {2016},
  journal = {arXiv preprint arXiv:1606.01540}
}
@article{srivastava2014dropout,
  title     = {Dropout: a simple way to prevent neural networks from overfitting},
  author    = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal   = {The journal of machine learning research},
  volume    = {15},
  number    = {1},
  pages     = {1929--1958},
  year      = {2014},
  publisher = {JMLR. org}
}
@article{krogh1991simple,
  title   = {A simple weight decay can improve generalization},
  author  = {Krogh, Anders and Hertz, John},
  journal = {Advances in neural information processing systems},
  volume  = {4},
  year    = {1991}
}
@article{sharma2017activation,
  title   = {Activation functions in neural networks},
  author  = {Sharma, Sagar and Sharma, Simone and Athaiya, Anidhya},
  journal = {Towards Data Sci},
  volume  = {6},
  number  = {12},
  pages   = {310--316},
  year    = {2017}
}
@article{schulman2017proximal,
  title   = {Proximal policy optimization algorithms},
  author  = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal = {arXiv preprint arXiv:1707.06347},
  year    = {2017}
}
@inproceedings{fedus2020revisiting,
  title        = {Revisiting fundamentals of experience replay},
  author       = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
  booktitle    = {International Conference on Machine Learning},
  pages        = {3061--3071},
  year         = {2020},
  organization = {PMLR}
}
@article{ha2018recurrent,
  title   = {Recurrent world models facilitate policy evolution},
  author  = {Ha, David and Schmidhuber, J{\"u}rgen},
  journal = {Advances in neural information processing systems},
  volume  = {31},
  year    = {2018}
}
@article{parisi2019continual,
  title     = {Continual lifelong learning with neural networks: A review},
  author    = {Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
  journal   = {Neural networks},
  volume    = {113},
  pages     = {54--71},
  year      = {2019},
  publisher = {Elsevier}
}
@article{hernandez2019understanding,
  title   = {Understanding multi-step deep reinforcement learning: A systematic study of the DQN target},
  author  = {Hernandez-Garcia, J Fernando and Sutton, Richard S},
  journal = {arXiv preprint arXiv:1901.07510},
  year    = {2019}
}
@article{skolik2022quantum,
  title     = {Quantum agents in the gym: a variational quantum algorithm for deep q-learning},
  author    = {Skolik, Andrea and Jerbi, Sofiene and Dunjko, Vedran},
  journal   = {Quantum},
  volume    = {6},
  pages     = {720},
  year      = {2022},
  publisher = {Verein zur F{\"o}rderung des Open Access Publizierens in den Quantenwissenschaften}
}
@techreport{moore1990efficient,
  title       = {Efficient memory-based learning for robot control},
  author      = {Moore, Andrew William},
  year        = {1990},
  institution = {University of Cambridge, Computer Laboratory}
}
@article{singh1996reinforcement,
  title     = {Reinforcement learning with replacing eligibility traces},
  author    = {Singh, Satinder P and Sutton, Richard S},
  journal   = {Machine learning},
  volume    = {22},
  number    = {1-3},
  pages     = {123--158},
  year      = {1996},
  publisher = {Springer}
}
@article{campbell2014car,
  title     = {Top down car},
  author    = {Campbell, Chris},
  year      = {2014},
  publisher = {iforce2d},
  url       = {http://www.iforce2d.net/b2dtut/top-down-car}
}
@article{stooke2018accelerated,
  title   = {Accelerated methods for deep reinforcement learning},
  author  = {Stooke, Adam and Abbeel, Pieter},
  journal = {arXiv preprint arXiv:1803.02811},
  year    = {2018}
}
@inproceedings{liang2018cirl,
  title     = {Cirl: Controllable imitative reinforcement learning for vision-based self-driving},
  author    = {Liang, Xiaodan and Wang, Tairui and Yang, Luona and Xing, Eric},
  booktitle = {Proceedings of the European conference on computer vision (ECCV)},
  pages     = {584--599},
  year      = {2018}
}
@article{rusu2016progressive,
  title   = {Progressive neural networks},
  author  = {Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  journal = {arXiv preprint arXiv:1606.04671},
  year    = {2016}
}
@article{yang2020multi,
  title     = {Multi-robot path planning based on a deep reinforcement learning DQN algorithm},
  author    = {Yang, Yang and Juntao, Li and Lingling, Peng},
  journal   = {CAAI Transactions on Intelligence Technology},
  volume    = {5},
  number    = {3},
  pages     = {177--183},
  year      = {2020},
  publisher = {Wiley Online Library}
}
@article{sallab2016end,
  title   = {End-to-end deep reinforcement learning for lane keeping assist},
  author  = {Sallab, Ahmad El and Abdou, Mohammed and Perot, Etienne and Yogamani, Senthil},
  journal = {arXiv preprint arXiv:1612.04340},
  year    = {2016}
}
@article{lo2019overcoming,
  title   = {Overcoming catastrophic interference in online reinforcement learning with dynamic self-organizing maps},
  author  = {Lo, Yat Long and Ghiassian, Sina},
  journal = {arXiv preprint arXiv:1910.13213},
  year    = {2019}
}
@article{mnih2015human,
  title     = {Human-level control through deep reinforcement learning},
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal   = {nature},
  volume    = {518},
  number    = {7540},
  pages     = {529--533},
  year      = {2015},
  publisher = {Nature Publishing Group}
}
@article{ladosz2022exploration,
  title     = {Exploration in deep reinforcement learning: A survey},
  author    = {Ladosz, Pawel and Weng, Lilian and Kim, Minwoo and Oh, Hyondong},
  journal   = {Information Fusion},
  year      = {2022},
  publisher = {Elsevier}
}
@article{gou2019dqn,
  title   = {DQN with model-based exploration: efficient learning on environments with sparse rewards},
  author  = {Gou, Stephen Zhen and Liu, Yuyang},
  journal = {arXiv preprint arXiv:1903.09295},
  year    = {2019}
}
@article{tsitsiklis1994asynchronous,
  title     = {Asynchronous stochastic approximation and Q-learning},
  author    = {Tsitsiklis, John N},
  journal   = {Machine learning},
  volume    = {16},
  pages     = {185--202},
  year      = {1994},
  publisher = {Springer}
}
@article{lecun2015deep,
  title     = {Deep learning},
  author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal   = {nature},
  volume    = {521},
  number    = {7553},
  pages     = {436--444},
  year      = {2015},
  publisher = {Nature Publishing Group UK London}
}
@inproceedings{nikishin2018improving,
  title     = {Improving stability in deep reinforcement learning with weight averaging},
  author    = {Nikishin, Evgenii and Izmailov, Pavel and Athiwaratkun, Ben and Podoprikhin, Dmitrii and Garipov, Timur and Shvechikov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
  booktitle = {Uncertainty in artificial intelligence workshop on uncertainty in Deep learning},
  year      = {2018}
}
@inproceedings{mai2021stability,
  title        = {Stability and convergence of stochastic gradient clipping: Beyond lipschitz continuity and smoothness},
  author       = {Mai, Vien V and Johansson, Mikael},
  booktitle    = {International Conference on Machine Learning},
  pages        = {7325--7335},
  year         = {2021},
  organization = {PMLR}
}

%

@article{joshua2018spinning,
  author = {Achiam, Joshua},
  title  = {{Spinning Up in Deep Reinforcement Learning}},
  year   = {2018}
}
@inproceedings{silver2014deterministic,
  title        = {Deterministic policy gradient algorithms},
  author       = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle    = {International conference on machine learning},
  pages        = {387--395},
  year         = {2014},
  organization = {Pmlr}
}
@article{andrychowicz2020matters,
  title   = {What matters in on-policy reinforcement learning? a large-scale empirical study},
  author  = {Andrychowicz, Marcin and Raichuk, Anton and Sta{\'n}czyk, Piotr and Orsini, Manu and Girgin, Sertan and Marinier, Raphael and Hussenot, L{\'e}onard and Geist, Matthieu and Pietquin, Olivier and Michalski, Marcin and others},
  journal = {arXiv preprint arXiv:2006.05990},
  year    = {2020}
}
@article{babaeizadeh2016reinforcement,
  title   = {Reinforcement learning through asynchronous advantage actor-critic on a gpu},
  author  = {Babaeizadeh, Mohammad and Frosio, Iuri and Tyree, Stephen and Clemons, Jason and Kautz, Jan},
  journal = {arXiv preprint arXiv:1611.06256},
  year    = {2016}
}
@book{sutton1998introduction,
  title     = {Introduction to reinforcement learning},
  author    = {Sutton, Richard S and Barto, Andrew G and others},
  volume    = {135},
  year      = {1998},
  publisher = {MIT press Cambridge}
}
@article{sutton2005reinforcement,
  title   = {Reinforcement Learning: An Introduction. A Bradford Book},
  author  = {Sutton, Richard S and Barto, Andrew G},
  journal = {IEEE Transactions on Neural Networks},
  volume  = {16},
  number  = {1},
  pages   = {285--286},
  year    = {2005}
}
